{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentlab.experiments.exp_utils import RESULTS_DIR\n",
    "from agentlab.analyze import inspect_results\n",
    "from agentlab.experiments.study import get_most_recent_study\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all summaries\n",
    "this will iterate over your RESULTS_DIR directory and create a summary of all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summaries = inspect_results.get_all_summaries(\n",
    "    RESULTS_DIR.resolve().parent / \"ICML-Neurips-final-run\", ignore_cache=False, ignore_stale=True\n",
    ")\n",
    "all_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results\n",
    "find the most recent study and load all summary information in a result dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this by your desired directory if needed.\n",
    "result_dir = get_most_recent_study(RESULTS_DIR, contains=None)\n",
    "\n",
    "print(result_dir)\n",
    "result_df = inspect_results.load_result_df(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various kind of reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic report using summarize\n",
    "# result_df = inspect_results.set_task_category_as_index(result_df)\n",
    "report = inspect_results.global_report(result_df)\n",
    "inspect_results.display_report(report)\n",
    "\n",
    "# some potentially useful flag analysis\n",
    "inspect_results.flag_report(report, metric=\"avg_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more stats\n",
    "report_stats = inspect_results.global_report(result_df, reduce_fn=inspect_results.summarize_stats)\n",
    "inspect_results.display_report(report_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # breakdown by error type\n",
    "# report = inspect_results.global_report(result_df, reduce_fn=inspect_results.report_different_errors)\n",
    "# inspect_results.display_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation study\n",
    "(TODO this might need some dedusting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ablation_report \u001b[38;5;241m=\u001b[39m inspect_results\u001b[38;5;241m.\u001b[39mablation_report(\u001b[43mresult_df\u001b[49m)\n\u001b[1;32m      2\u001b[0m inspect_results\u001b[38;5;241m.\u001b[39mdisplay_report(ablation_report)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": [
    "ablation_report = inspect_results.ablation_report(result_df)\n",
    "inspect_results.display_report(ablation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Variables\n",
    "Prints all constants and first 3 unique values of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_results.report_constant_and_variables(result_df, show_stack_traces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "report = inspect_results.error_report(result_df, max_stack_trace=2, use_log=True)\n",
    "# display(Markdown(report))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect_results.error_report_detailed(result_df, max_stack_trace=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
